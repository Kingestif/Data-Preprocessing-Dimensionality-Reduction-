# -*- coding: utf-8 -*-
"""Data Preprocessing & Dimensionality Reduction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uUr0CLIvPKQeJhnLEcHcqF2_84ZgximW

# **Titanic Dataset**
"""

import pandas as pd

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('train.csv')

# Preview the dataset
print("First 5 rows:\n", df.head())

# Check for missing values
print("\nMissing Values:\n")
missing_cols = df.isnull().sum()
missing_cols = missing_cols[missing_cols > 0]
print(missing_cols)

"""## 1. Handling missing data"""

# 1: Impute missing values
# Numerical columns: Fill with median
df['Age'] = df['Age'].fillna(df['Age'].median())

# 2: Mode for Categorical (embarked & cabin)
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])
df['Cabin'] = df['Cabin'].fillna(df['Cabin'].mode()[0])

# Check for missing values again must be 0 for all
print("\nMissing Values:\n")
print(df.isnull().sum())

"""## 2. Encoding"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

# 1. Label encoding (Sex)
# AUTOMATIC LABEL ENCODING
encoder = LabelEncoder()
df['Sex_encoded'] = encoder.fit_transform(df['Sex'])
print(df[['Name','Sex', 'Sex_encoded']])

# 2. AUTOMATIC ONE-HOT ENCODING (Cabin, Embarked)
df_encoded = pd.get_dummies(df, columns=['Embarked', 'Cabin'], prefix=['Embark', 'Cabin'])
print("\nAutomatic One-Hot Encoding Result:")
display(df_encoded)

"""## 3. Outlier detection"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate Q1, Q3, and IQR for the 'Fare' column
Q1 = df['Fare'].quantile(0.25)
Q3 = df['Fare'].quantile(0.75)
IQR = Q3 - Q1

# Identify outliers using the IQR method
outliers = df[(df['Fare'] < Q1 - 1.5 * IQR) | (df['Fare'] > Q3 + 1.5 * IQR)]

print("DataFrame Shape:", df.shape)

print("\nQ1:", Q1)
print("Q3:", Q3)
print("IQR:", IQR)
print("\nOutliers Shape:", outliers.shape) #number of outliers from our original data


#Visual plotting
sns.boxplot(df['Fare'])
plt.title("Boxplot of Fare (IQR Method)")
plt.show()

"""## 4. Feature Scaling"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler


scaler = StandardScaler()
numeric_features = ['Age', 'Fare']
df[numeric_features] = scaler.fit_transform(df[numeric_features])

# Check the result
print(df[numeric_features].describe())

"""## 5. Again Outlier Detection after feature scaling"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Detect outliers based on Z-score
z_threshold = 3

# For each numeric column, find rows where |z| > 3
for col in numeric_features:
    outliers = df[np.abs(df[col]) > z_threshold]
    print(f"\nOutliers in '{col}' after scaling:")
    display(outliers[[col]])


#Visual plotting
sns.boxplot(df['Fare'])
plt.title("Boxplot of Fare (IQR Method)")
plt.show()

"""## 6. Train Test Split"""

from sklearn.model_selection import train_test_split

X = df.drop('Survived', axis=1)
y = df['Survived']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Train set:", X_train.shape)
print("Test set:", X_test.shape)

"""# **House Price Prediction**"""

import pandas as pd

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('train.csv')

# Preview the dataset
print("First 5 rows:\n", df.head())

# Check for missing values
print("\nMissing Values:\n")
missing_cols = df.isnull().sum()
missing_cols = missing_cols[missing_cols > 0]
print(missing_cols)

"""## Handling Missing Data"""

# 1: Impute missing values
# Numerical columns: Fill with median
df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].median())
df['MasVnrArea'] = df['MasVnrArea'].fillna(df['MasVnrArea'].median())
df['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['GarageYrBlt'].median())

# 2: Mode for Categorical (embarked & cabin)
cat_none = [
    'Alley', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',
    'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',
    'PoolQC', 'Fence', 'MiscFeature'
]

# If The feature is physically missing in reality for ex some houses dont have Alley access so used None to represent that
df[cat_none] = df[cat_none].fillna("None")

# Here electrical is missing not becuase the house dont have one but some entry issue so used mode to fill it
df["Electrical"] = df["Electrical"].fillna(df["Electrical"].mode()[0])

# Check for missing values again must be 0 for all
print("\nMissing Values:\n")
missing_cols = df.isnull().sum()
missing_cols = missing_cols[missing_cols > 0]
print(missing_cols)

"""## Encoding"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

ordinal_features = {
    'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'BsmtQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'BsmtCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'BsmtExposure': ['None', 'No', 'Mn', 'Av', 'Gd'],
    'BsmtFinType1': ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],
    'BsmtFinType2': ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],
    'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'FireplaceQu': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'GarageFinish': ['None', 'Unf', 'RFn', 'Fin'],
    'GarageQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'GarageCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'PoolQC': ['None', 'Fa', 'TA', 'Gd', 'Ex'],
    'Fence': ['None', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'],
    'PavedDrive': ['N', 'P', 'Y'],
    'Functional': ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ']
}

nominal_features = [
    'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',
    'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',
    'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',
    'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'CentralAir',
    'Electrical', 'GarageType', 'MiscFeature', 'SaleType', 'SaleCondition'
]

# 1. MANUAL LABEL ENCODING since we have manually ordered the dictionaryy
for feature, order in ordinal_features.items():
    order_mapping = {category: i for i, category in enumerate(order)}
    df[feature] = df[feature].map(order_mapping)

print(df.columns.tolist())

print("\nEncoded Ordinal Columns Sample:")
print(df[list(ordinal_features.keys())].head())

# 2. AUTOMATIC ONE-HOT ENCODING for Nominal
df = pd.get_dummies(df, columns=nominal_features, drop_first=True)

print("\nAutomatic One-Hot Encoding Result:")
display(df)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler

# Step 1: Choose numeric columns (exclude target)
numerical_cols = df.select_dtypes(include=[np.number]).drop('SalePrice', axis=1).columns

# Step 2: Initialize and fit scaler
scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

print(df[numerical_cols])

"""## PCA Dimensionality Reduction"""

from sklearn.decomposition import PCA

# Step 1: Separate target
X = df.drop('SalePrice', axis=1)
y = df['SalePrice']

# Step 2: Apply PCA
pca = PCA(n_components=0.95)  # Keep 95% of variance
X_pca = pca.fit_transform(X)

# Step 3: Check result
print("Original shape:", X.shape)
print("Reduced shape:", X_pca.shape)

"""## Train Test Split"""

from sklearn.model_selection import train_test_split

# We used X_pca reduced features not original bulky features, first Split the PCA-transformed features and the target variable
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

"""# **Iris Species Dataset**"""

import seaborn as sns
import pandas as pd

# Load the iris dataset
df = sns.load_dataset('iris')

# Preview the dataset
print(df.head())

"""## Check for Missing values"""

# Check if there is null values
print(df.isnull().sum())

"""## Encoding Categorical Column"""

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
df['species_encoded'] = encoder.fit_transform(df['species'])

print(df)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler

features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df[features])
df[features] = scaled_features

print(df[features])

"""## Train Test Split"""

from sklearn.model_selection import train_test_split

X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]
y = df['species_encoded']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""# **Diabetes Health Indicators**"""

import pandas as pd
from google.colab import files

uploaded = files.upload()

df = pd.read_csv('diabetes_binary_5050split_health_indicators_BRFSS2015.csv')


# Preview the dataset
print("First 5 rows:\n", df.head())

# Check for missing values
print("\nMissing Values:\n")
missing_cols = df.isnull().sum()
missing_cols = missing_cols[missing_cols > 0]
print(missing_cols)

"""## No Missing Values, No Encoding needed since features are in numerical form

## Scaling
"""

from sklearn.preprocessing import StandardScaler

X = df.drop('Diabetes_binary', axis=1)
y = df['Diabetes_binary']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)
print(X_scaled_df.head())

"""## PCA For Dimension Reduction"""

from sklearn.decomposition import PCA

# Step 1: Separate target
X = df.drop('Diabetes_binary', axis=1)
y = df['Diabetes_binary']

# Step 2: Apply PCA
pca = PCA(n_components=0.95)  # Keep 95% of variance
X_pca = pca.fit_transform(X)

# Step 3: Check result
print("Original shape:", X.shape)
print("Reduced shape:", X_pca.shape)

"""## Train Test Split"""

from sklearn.model_selection import train_test_split

# Train-test split after PCA
X_train, X_test, y_train, y_test = train_test_split(
    X_pca, y, test_size=0.2, random_state=42, stratify=y
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)